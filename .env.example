# MCP Server Configuration
HOST=0.0.0.0
PORT=8051
TRANSPORT=sse

# ===== EMBEDDING PROVIDERS =====
# Choose your embedding provider: openai, cohere, ollama, huggingface, auto
EMBEDDING_PROVIDER=auto

# OpenAI Embeddings (optional - only if using OpenAI embeddings)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Cohere Embeddings (optional)
COHERE_API_KEY=your_cohere_api_key_here
COHERE_EMBEDDING_MODEL=embed-english-v3.0
COHERE_USE_V2=false  # Set to true for multimodal support with embed-v4.0

# Ollama Embeddings (optional - for local embeddings)
ENABLE_OLLAMA=false
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# HuggingFace Embeddings (optional)
ENABLE_HUGGINGFACE=false
HUGGINGFACE_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
HUGGINGFACE_USE_API=false
HUGGINGFACE_API_TOKEN=your_hf_api_token_here  # Only if USE_API=true
HUGGINGFACE_DEVICE=cpu

# ===== LLM PROVIDERS (NEW - OPEN SOURCE) =====
# Choose your LLM provider: huggingface, ollama, auto
LLM_PROVIDER=auto

# Ollama LLM (recommended for local inference)
ENABLE_OLLAMA_LLM=true
OLLAMA_LLM_MODEL=mistral:instruct

# HuggingFace LLM
ENABLE_HUGGINGFACE_LLM=true
HUGGINGFACE_LLM_MODEL=microsoft/Phi-3-mini-4k-instruct
HUGGINGFACE_LLM_USE_API=false
HUGGINGFACE_LLM_DEVICE=cpu
HUGGINGFACE_LLM_4BIT=false
HUGGINGFACE_LLM_8BIT=false

# ===== RAG STRATEGIES =====
USE_CONTEXTUAL_EMBEDDINGS=false
USE_HYBRID_SEARCH=false
USE_AGENTIC_RAG=false
USE_RERANKING=false

# ===== DATABASE CONFIGURATION =====
SUPABASE_URL=your_supabase_project_url_here
SUPABASE_SERVICE_KEY=your_supabase_service_key_here

# ===== EXAMPLE CONFIGURATIONS =====
# Uncomment one of these sections for quick setup:

# === Option 1: Minimal Local Setup (Ollama) ===
# EMBEDDING_PROVIDER=ollama
# ENABLE_OLLAMA=true
# LLM_PROVIDER=ollama
# ENABLE_OLLAMA_LLM=true

# === Option 2: GPU-Accelerated Setup ===
# EMBEDDING_PROVIDER=huggingface
# ENABLE_HUGGINGFACE=true
# HUGGINGFACE_DEVICE=cuda
# LLM_PROVIDER=huggingface
# ENABLE_HUGGINGFACE_LLM=true
# HUGGINGFACE_LLM_DEVICE=cuda
# HUGGINGFACE_LLM_4BIT=true

# === Option 3: API-Based Setup (Lower Resource Usage) ===
# EMBEDDING_PROVIDER=cohere
# COHERE_API_KEY=your_key_here
# LLM_PROVIDER=huggingface
# HUGGINGFACE_LLM_USE_API=true
# HUGGINGFACE_API_TOKEN=your_token_here